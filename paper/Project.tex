\documentclass{article}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}

\usepackage{caption}
\captionsetup[lstlisting]{font={small,tt}}

\usepackage{fancyvrb}

\usepackage{graphicx}
\graphicspath{ {./gallery/} }

\author{
	Dub\'e, Fletcher\\
	\texttt{\#921441405}
	\and
	Redivo, Leonardo\\
	\texttt{\#918072175}
	\and
	Voragen, Richard\\
	\texttt{\#917981018}
}
\title{Parallel Mandelbrot: \\An exercise in CUDA and OpenMP}

\begin{document}
\maketitle

\includegraphics[width=\textwidth]{gallery/overview.png}

\section{Introduction} \label{sec:intro}
    The \emph{Mandelbrot set} is the set of complex numbers $c$ for which the function
    \begin{equation} \label{eq:mandelbrot}
    f_c(z) = z^2 + c
    \end{equation}
    does not diverge to infinity when iterated from $z = 0$ \cite{mandel_wiki}. In simple terms: choose a complex number $c$, and let $z = 0$. Compute the result, and let $z$ be that result. Continue iterating until the function results in a value that exceeds some threshold, or until some large number of iterations have been computed. If the function exceeds the threshold, the function diverges and $c$ is \emph{not} in the set. Otherwise, the function does not diverge and $c$ is included in the set.

\section{Parallel Design}
    To render the Mandelbrot set on the complex plane, each complex number is represented by a pixel. A pixel is colored black if it is in the Mandelbrot set. Otherwise, it is given a color based on how many iterations before the value of the function exceeded the threshold. Given that each pixel can be colored independent of other pixels, the problem is embarassingly parallel.

    \subsection{Partitioning}
        The essential task in this problem is to color a single pixel as described above. Therefore, the color of many different pixels is computed in parallel.

    \subsection{Communication}
        The primitive tasks are completely independent of each other, so the tasks need not communicate with each other. However, the result (pixel color) of each task must be delivered to some combined data structure that represents the color of each point on the plane.

    \subsection{Agglomeration}
        Each primitive task requires no information except the complex number it represents. Additionally, the only work done is to repeatedly apply function \ref{eq:mandelbrot}. Therefore, it is not appropriate to agglomerate any tasks.

    \subsection{Mapping}
        The problem is to be broken up by sets of pixels: each worker takes some set of pixels to color.


\section{Implementation} \label{sec:impl}
    The base source code was taken from Macalester College \cite{mandel_orig}, which included a basic CUDA implementation and simple X11 display, as well as a benchmarking utility.

    \subsection{Serial}
        As a baseline, a serial version of the Mandelbrot algorithm was implemented (Appendix \ref{appendix:serial}.) This version has one major improvement over a na\"ive implementation. Instead of utilizing imaginary numbers, a complex number is represented by the variables \verb|cr| and \verb|ci|, which represent the real and imaginary portion respectively. The imaginary portion is treated as though it is multiplied by $\sqrt{-1}$. This eliminates any use of the expensive \verb|cpow| and \verb|cabs| functions, thus greatly improving the runtime of the serial code.

    \subsection{OpenMP}
        To compute Mandelbrot on multiple CPU threads, the OpenMP \verb|parallel for| pragma is used (Appendix \ref{appendix:omp}.) The \verb|schedule(runtime)| clause allows the scheduling strategy to be set by changing the command-line environment variable \verb|OMP_SCHEDULE|. The \verb|dynamic| schedule was hypothesized to be optimal, because some blocks of pixels are easier to compute than others, because they converge faster. In testing, the \verb|guided| strategy resulted in the most speedup. This was a surprising result; there is not an obvious ``reverse taper'' of work to be done per complex number (later computations shouldn't necessarily be much more taxing than earlier ones.)

    \subsection{CUDA}
        The original CUDA code was modified to use the complex abstraction above (Appendix \ref{appendix:cuda}.) An equal number of pixels is assigned to each thread sequentially (block partitioning), and each thread computes the color of each pixel assigned to it. These computed values are stored in a shared memory array \verb|colors[]|, which is then copied to host memory after the threads finish.

\section{Experimentation}
    The primary focus of experimentation was to examine the effect of different CUDA thread configurations on speedup. Specifically, the number of blocks $b$ and the number of threads per block $t$ were varied. In order to facilitate fair comparison, these values are varied such that the total number of threads $t \times b$ remained constant. Testing was done on an NVIDIA 1050; any optimal values may vary among GPUs.

    In each case, fifty $1\,600 \times 1\,600$ instances were run. The total number of threads was limited to 524\,288 because device memory is allocated for each thread. The raw output from test runs is in Appendix \ref{appendix:testdata}.

    \def\boxit#1{%
      \smash{\fboxsep=0pt\llap{\rlap{\fbox{\strut\makebox[#1]{}}}~}}\ignorespaces
    }

    \renewcommand{\arraystretch}{1.5}
    \begin{table}[ht]
    \centering
    \caption{Speedup based on thread configuration}
    \begin{tabular}{r | r || r}\label{tab:speedup}
    \# of blocks & threads/block & speedup \\ \hline
    512                 & 1\,024    & 17.99 \\
    1\,024              & 512       & 18.24 \\
    \boxit{150pt}2\,048 & 256       & 18.35 \\
    4\,096              & 128       & 18.13 \\
    8\,192              & 64        & 18.16 \\
    16\,384             & 32        & 18.15
    \end{tabular}
    \end{table}

    The results in Table \ref{tab:speedup} suggest that a balance between the number of blocks and the number of threads per block is optimal. Specifically, using 2\,048 blocks and 256 threads per block yielded a 18.35x speedup over the serial version. Any improvement is fairly minimal, with a $\sim 2\%$ difference between the fastest and the slowest thread configurations; though, running higher-resolution instances (more work) would amplifiy any minor speedup differences.

    \subsection{NVIDIA Visual Profiler}
    The NVIDIA Visual Profiler is a tool that allows the developers of CUDA code to get important insights on how the kernel is running on the device. With this tool, one can learn many details about the kernel including (but not limited to): the amount of time in picoseconds the kernel and the memory transfer (from host to device and vice-versa) take to run; how many registers, threads, and blocks were used by the kernel; the PTX (NVIDIA's assembly language) version of the CUDA code; as well as the amount of inactive and predicated threads. Along with these features, the profiler also gives the developer a detailed analysis of how the kernel is performing and where the bottlenecks are. After running our implementation of the mandelbrot kernel through the profiler, the analysis informed us that the main bottleneck the kernel suffered from was the overusage of functional units, the different logical units present in the GPU. Our code was primarily overusing the FP64 functional unit, responsible for performing operations related to the $double$ type, as this is the primary type used during the computation of the mandelbrot set. From this information, we we're able to minimize the usage of FP64 operations by truncating some operations in the original kernel into less, yet more complicated, instructions. We also attempted to use the less accurate FP32 type, $float$, however this proved to be detrimental during the calculation of the mandelbrot set. With these changes to the instructions we were able to generate and extra 1-2x speedup from the serial version, on top of the 18x speedup the program previously had, bringing our total speedup over the serial version to 20x (values based on NVIDIA 1050, other GPU may show different values). We believe that these performance improvements come primarily from the reduction in instruction count of the program, as well as the reduction of usage of the FP64 functional units.

\pagebreak    
\appendix
    \section{Contributions}
        \begin{itemize}
        	\item \emph{Fletcher}: \LaTeX{}; CUDA optimization and testing; PowerPoint.

        	\item \emph{Leonardo}: CUDA optimization and testing; PowerPoint; \LaTeX{}.

        	\item \emph{Richard}: UX (X11, zoom and pan), serial, and OpenMP in \ref{appendix:mandelbrot.cu}; assisted in optimization testing and data collection; \LaTeX{}; PowerPoint.
        \end{itemize}

    \newcommand{\centeredimage}[2]{
        \vspace*{\fill}%
        \noindent%
        \makebox[\textwidth]{\includegraphics[#1]{#2}}%
        \vspace*{2cm}%
    }

    \pagebreak
    \section{Gallery}
        \centeredimage{width=0.75\paperwidth}{gallery/Image2.png}
        \clearpage
        \centeredimage{width=0.75\paperwidth}{gallery/Image3.png}
        \clearpage
        \centeredimage{width=0.75\paperwidth}{gallery/Image4.png}
        \clearpage
        \centeredimage{width=0.75\paperwidth}{gallery/Image5.png}
        \clearpage
        \centeredimage{width=0.75\paperwidth}{gallery/Image6.png}
        \clearpage

    \section{Code}
        \lstset{language=C,stringstyle=\ttfamily, showstringspaces=false, numbers=left, frame=single, framexrightmargin=0pt, columns=fullflexible, breaklines=true, breakatwhitespace=true}

        \subsection{Serial Mandelbrot}\label{appendix:serial}
\begin{lstlisting}
uint32_t mandel_double_single(double cr, double ci, int max_iter) {
    double zr = 0, zi = 0, zrsqr = 0, zisqr = 0;

    uint32_t i;

    for (i = 0; i < max_iter; i++){
		zi = zr * zi;
		zi += zi;
		zi += ci;
		zr = zrsqr - zisqr + cr;
		zrsqr = zr * zr;
		zisqr = zi * zi;
		
    //the fewer iterations it takes to diverge, the farther from the set
		if (zrsqr + zisqr > 4.0) break;
    }
    return i;
}
\end{lstlisting}

        \subsection{OpenMP Mandelbrot}\label{appendix:omp}
\begin{lstlisting}
void mandel_single(uint32_t *counts, double xmin, double ymin,
            double step, int max_iter, int dim, uint32_t *colors) {
    int i, x, y;
    double cr, ci;
    if (Version == 1){
        # pragma omp parallel for num_threads(CPU_CORES) \
            schedule(runtime) private(i,x,y,cr,ci) \
            shared(counts, dim, colors, xmin, ymin, step, max_iter)
        for (i = 1; i < dim*dim; i++){
            x = i % dim;
            y = i / dim;
            cr = xmin + x * step;
            ci = ymin + y * step;
            counts[y * dim + x]  = colors[mandel_double_single(cr, ci, max_iter)];
        }
    } else {
        for (i = 1; i < dim*dim; i++){
            x = i % dim;
            y = i / dim;
            cr = xmin + x * step;
            ci = ymin + y * step;
            counts[y * dim + x]  = colors[mandel_double_single(cr, ci, max_iter)];
        }
    }
}
\end{lstlisting}

        \subsection{CUDA Mandelbrot}\label{appendix:cuda}
\begin{lstlisting}
__global__ void mandel_kernel(uint32_t *counts, double xmin, double ymin,
            double step, int max_iter, int dim, uint32_t *colors) {
    int pix_per_thread = dim * dim / (gridDim.x * blockDim.x);
    int tId = blockDim.x * blockIdx.x + threadIdx.x;
    int offset = pix_per_thread * tId;
    for (int i = offset; i < offset + pix_per_thread; i++){
        int x = i % dim;
        int y = i / dim;
        double cr = xmin + x * step;
        double ci = ymin + y * step;
        counts[y * dim + x]  = colors[mandel_double(cr, ci, max_iter)];
    }
    if (gridDim.x * blockDim.x * pix_per_thread < dim * dim
            && tId < (dim * dim) - (blockDim.x * gridDim.x)){
        int i = blockDim.x * gridDim.x * pix_per_thread + tId;
        int x = i % dim;
        int y = i / dim;
        double cr = xmin + x * step;
        double ci = ymin + y * step;
        counts[y * dim + x]  = colors[mandel_double(cr, ci, max_iter)];
    }
}
\end{lstlisting}

        \subsection{Entire Program} \label{appendix:mandelbrot.cu}
            \lstinputlisting[caption={mandelbrot.cu}]{../mandelbrot.cu}

    \section{Test runs} \label{appendix:testdata}
        \lstset{language={},numbers=none}
        \lstinputlisting{output.txt}

\bibliography{biblio} 
\bibliographystyle{ieeetr}

\end{document}